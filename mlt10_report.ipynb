{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification of Depression and Gender by Vocal Characteristics\n",
    "In this project, we will train a Random Forest Classifier to determine whether a participant has depression based on speech data from a clinical interview. The dataset contains 107 participants, 63 of which are male and 44 of which are female. Of these participants, 30 have depression (17 female, 14 male). By varying the weights and features included in the model training, we will demonstrate how differing analysis techniques can have significant impact on the model's classification performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** MATH OPERATIONS ****\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# **** DATA MANIPULATION ****\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general functions\n",
    "color_dict = {\n",
    "    'all features': 'mediumpurple',\n",
    "    'top performing features (depression)': 'darkviolet',\n",
    "    'top perfoming features (gender)': 'plum',\n",
    "    're-weighted features': 'indigo'\n",
    "}\n",
    "def load_features(path, labels_df):\n",
    "    dataframes = []\n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        if file.split('.')[1] != 'csv':\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(path, file), header=None)\n",
    "        participant_id = int(file.split(\"_\")[1].split(\".\")[0]) # Get ID from file name\n",
    "        df[\"Participant_ID\"] = participant_id\n",
    "        dataframes.append(df)\n",
    "        \n",
    "    # Combine into a single dataframe\n",
    "    data_df = pd.concat(dataframes)\n",
    "    return data_df.merge(labels_df, on=\"Participant_ID\")\n",
    "\n",
    "def analyze_results(test_df, display_results=True, pred_label='Depression', threshold = 0.05):\n",
    "    # Group by participant and average predictions \n",
    "    participant_predictions = test_df.groupby(\"Participant_ID\")[\"predictions\"].mean()\n",
    "\n",
    "    participant_predictions_binarized = (participant_predictions >= threshold).astype(int)\n",
    "\n",
    "    # Join aggregated predictions back with the depression labels\n",
    "    participant_labels = test_df.groupby(\"Participant_ID\")[pred_label].first()\n",
    "\n",
    "    # Filter data by gender\n",
    "    male_participants = test_df[test_df[\"Gender\"] == 1][\"Participant_ID\"].unique()\n",
    "    female_participants = test_df[test_df[\"Gender\"] == 0][\"Participant_ID\"].unique()\n",
    "\n",
    "    # Calculate accuracies for all, male, and female participants\n",
    "    all_metrics_depression = calculate_accuracy(participant_labels, \n",
    "                                                participant_predictions_binarized, \"All participants\",\n",
    "                                                display_results=display_results)\n",
    "    if display_results:\n",
    "        print(\"\")\n",
    "    male_metrics = calculate_accuracy(participant_labels.loc[male_participants],\n",
    "                                       participant_predictions_binarized.loc[male_participants], \"Male participants\",\n",
    "                                       display_results=display_results)\n",
    "    if display_results:\n",
    "        print(\"\")\n",
    "    female_metrics = calculate_accuracy(participant_labels.loc[female_participants], \n",
    "                                        participant_predictions_binarized.loc[female_participants],\n",
    "                                          \"Female participants\", display_results=display_results)\n",
    "\n",
    "    # Calculate EO\n",
    "    eo = (1 - abs(male_metrics[\"tpr\"] - female_metrics[\"tpr\"]))\n",
    "    if display_results:\n",
    "        print(f\"Equality of Opportunity (EO): {eo:.2f}\")\n",
    "\n",
    "    return all_metrics_depression, male_metrics, female_metrics, eo\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred, group, display_results=True):\n",
    "    # Calculate Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate Balanced Accuracy\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Extract TP, FP, TN, FN\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    tpr = tp / (tp + fn) # True Positive Rate\n",
    "    tnr = tn / (tn + fp) # True Negative Rate\n",
    "    fpr = fp / (fp + tn) # False Positive Rate\n",
    "    fnr = fn / (fn + tp) # False Negative Rate\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"balanced_accuracy\": balanced_accuracy,\n",
    "        \"tpr\": tpr,\n",
    "        \"tnr\": tnr,\n",
    "        \"fpr\": fpr,\n",
    "        \"fnr\": fnr\n",
    "    }\n",
    "    if display_results:\n",
    "        print(f\"Metrics for {group}:\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Balanced Accuracy: {balanced_accuracy:.2f}\")\n",
    "        print(f\"True Positive Rate (TPR): {tpr:.2f}\")\n",
    "        print(f\"True Negative Rate (TNR): {tnr:.2f}\")\n",
    "        print(f\"False Positive Rate (FPR): {fpr:.2f}\")\n",
    "        print(f\"False Negative Rate (FNR): {fnr:.2f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_random_forest(df:pd.DataFrame, classification_feat:str):\n",
    "    # split into validation and train\n",
    "    train_df, validation_df = train_test_split(df, test_size=.3)\n",
    "    train_feat = train_df[classification_feat].values.tolist()\n",
    "    validation_feat = validation_df[classification_feat].values.tolist()\n",
    "    train_arr = train_df.drop(columns=[classification_feat]).to_numpy()\n",
    "    validation_arr = validation_df.drop(columns = [classification_feat]).to_numpy()\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    # clf = RandomForestClassifier(num_trees, random_state=42, class_weight='balanced', max_depth=max_depth)\n",
    "    clf.fit(train_arr, train_feat)\n",
    "    pred = clf.predict(validation_arr)\n",
    "    acc = accuracy_score(validation_feat, pred)\n",
    "    # return model with best classification accuracy\n",
    "    return clf\n",
    "\n",
    "def bar_graph(vals:dict, measures:list, title:str):\n",
    "    x = np.arange(len(measures))  # the label locations\n",
    "    width = 0.2  # the width of the bars\n",
    "    multiplier = 0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    for attribute, measurement in vals.items():\n",
    "        offset = width * multiplier\n",
    "        rects = ax.bar(x + offset, measurement, width, label=attribute, color=color_dict[attribute])\n",
    "        ax.bar_label(rects, padding=5)\n",
    "        multiplier += 1\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x + width, measures)\n",
    "    ax.legend(loc='upper left', ncols=4)\n",
    "    ax.set_ylim(0, 1)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in dataset & labels\n",
    "labels_df = pd.read_csv(\"labels.csv\")\n",
    "train_df = load_features(\"features_train\", labels_df)\n",
    "test_df = load_features(\"features_test\", labels_df)\n",
    "# append feature names\n",
    "features = pd.read_csv('feature_description.csv',\n",
    "                    encoding = 'ISO-8859-1', \n",
    "                    names=['feature', 'description'])['feature'].values.tolist()\n",
    "col_names = {x:features[x] for x in range(88)}\n",
    "train_df.rename(columns=col_names, inplace=True)\n",
    "test_df.rename(columns=col_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (1a) Classification of Gender and Depression\n",
    "TODO Fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (a.i) Depression Classification\n",
    "TODO Fill in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have these values at the end \n",
    "all_metrics_depression = 0\n",
    "male_metrics_depression = 0\n",
    "female_metrics_depression = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (a.ii) Gender Classification\n",
    "TODO fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have these values at the end\n",
    "all_metrics_gender = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (1b) Depression Feature Selection\n",
    "In this section, we will repeat the same depression classification as above on only the features with the strongest correlation with depression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform depression classification on the data\n",
    "correlation_tups = []\n",
    "for col in train_df.columns:\n",
    "    if col in ['Participant_ID', 'Depression']:\n",
    "        continue\n",
    "    correlation_tups.append((col, train_df[col].corr(train_df['Depression'])))\n",
    "correlation_tups = sorted(correlation_tups, key=lambda x: abs(x[1]), reverse=True)\n",
    "top_twenty_feats = {correlation_tups[x][0]:round(correlation_tups[x][1], 3)  for x in range(20)}\n",
    "print('Top twenty features correlated with depression: \\n')\n",
    "for key, val in top_twenty_feats.items():\n",
    "    print(f'{key}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The participants with depression more frequently spoke loudly (loudness_sma3) and with more fluctuations (spectralFlux). There is also slight correlation between the vocal range (semitone) and the speaker's depression. Since the majority of participants with depression were female, there is a correlation between gender and depression, as well.\n",
    "\n",
    "We will now train a Random Forest Classifier to predict depression using only the features most strongly correlated with depression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on filtered features\n",
    "class_acc_dep_fs, bca_dep_fs, male_class_acc_dep_fs, male_bca_dep_fs, female_class_acc_dep_fs, female_bca_dep_fs, eo_dep_fs = [],[],[],[],[], [], []\n",
    "for n in range(10, 51, 5):\n",
    "    # select top performing features\n",
    "    features = [correlation_tups[x][0] for x in range(n)] + ['Depression']\n",
    "    filtered_df = train_df.loc[:, features]\n",
    "    # build random forest on these features\n",
    "    best_rand_forest = train_random_forest(filtered_df, 'Depression')\n",
    "    filtered_test = test_df.loc[:, features]\n",
    "    predictions = best_rand_forest.predict(filtered_test.drop(columns=['Depression']).to_numpy())\n",
    "    # store accuracies\n",
    "    test_df_pred = test_df.copy()\n",
    "    test_df_pred['predictions'] = predictions\n",
    "    x = test_df_pred.columns.values.tolist()\n",
    "    all_res, male_res, female_res, eo_res = analyze_results(test_df_pred, display_results=False)\n",
    "    class_acc_dep_fs.append(all_res['accuracy'])\n",
    "    bca_dep_fs.append(all_res['balanced_accuracy'])\n",
    "    male_class_acc_dep_fs.append(male_res['accuracy'])\n",
    "    male_bca_dep_fs.append(male_res['balanced_accuracy'])\n",
    "    female_class_acc_dep_fs.append(female_res['accuracy'])\n",
    "    female_bca_dep_fs.append(female_res['balanced_accuracy'])\n",
    "    eo_dep_fs.append(eo_res)\n",
    "\n",
    "\n",
    "# plot results\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "num_feats = range(10, 51, 5)\n",
    "ax.plot(num_feats, class_acc_dep_fs, label = \"Classification Accuracy\", color='darkviolet') \n",
    "ax.plot(num_feats, male_class_acc_dep_fs, label = \"Male Classification Accuracy\", color='royalblue') \n",
    "ax.plot(num_feats, female_class_acc_dep_fs, label = \"Female Classification Accuracy\", color='violet') \n",
    "ax.plot(num_feats, bca_dep_fs, label = \"Balanced Classification Accuracy\", color='darkviolet', linestyle='dashed') \n",
    "ax.plot(num_feats, male_bca_dep_fs, label = \"Male Balanced Classification Accuracy\", color='royalblue', linestyle='dashed') \n",
    "ax.plot(num_feats, female_bca_dep_fs, label = \"Female Balanced Classification Accuracy\", color='violet', linestyle='dashed') \n",
    "ax.plot(num_feats, eo_dep_fs, label = \"Equality of Opportunity\", color='darkviolet', linestyle='dotted')\n",
    "ax.set_title('Depression Classification Metrics Over Feature Selection')\n",
    "ax.set_xlabel('Number of Features Analyzed')\n",
    "ax.set_ylabel('Classification Accuracy')\n",
    "fig.tight_layout()\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.15, 0.5, 0.5, 0.5)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the depression classification accuracy measures over male and female participants as the number of features included in the model increase. With minimal (10) features included in the model training, the Random Forest Classifier performs much better on female participants than on male participants. As more features (with decreasing correlation) increase, the classification accuracy on male participants increases, while the classification accuracy on female participants decreases. As the number of features increase, the equality of opportunity nears 60% and the overall classification and balanced classification accuracy nears 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind = bca_dep_fs.index(max(bca_dep_fs))\n",
    "best_male_ind = male_bca_dep_fs.index(max(male_bca_dep_fs))\n",
    "best_female_ind = female_bca_dep_fs.index(max(female_bca_dep_fs))\n",
    "measures = ['Acc', 'BCA', 'Male Acc', 'Male BCA', 'Female Acc', 'Female BCA', 'EO']\n",
    "all_metrics_depression = {x:round(y, 2) for x,y in all_metrics_depression.items()}\n",
    "male_metrics = {x:round(y, 2) for x,y in male_metrics_depression.items()}\n",
    "female_metrics = {x:round(y, 2) for x,y in female_metrics_depression.items()}\n",
    "class_acc = [round(x, 2) for x in class_acc_dep_fs]\n",
    "male_class_acc = [round(x, 2) for x in male_class_acc_dep_fs]\n",
    "female_class_acc = [round(x, 2) for x in female_class_acc_dep_fs]\n",
    "bca = [round(x, 2) for x in bca_dep_fs]\n",
    "male_bca = [round(x, 2) for x in male_bca_dep_fs]\n",
    "female_bca = [round(x, 2) for x in female_bca_dep_fs]\n",
    "vals = {\n",
    "    'all features': [all_metrics_depression['accuracy'], all_metrics_depression['balanced_accuracy'], \n",
    "                     male_metrics['accuracy'], male_metrics['balanced_accuracy'], \n",
    "                     female_metrics['accuracy'], female_metrics['balanced_accuracy'], .6],\n",
    "    'top performing features (depression)': [class_acc[best_ind], bca[best_ind], \n",
    "                          male_class_acc[best_male_ind], male_bca[best_male_ind],\n",
    "                          female_class_acc[best_female_ind], female_bca[best_female_ind], eo_dep_fs[best_ind]],     \n",
    "}\n",
    "bar_graph(vals, measures, 'Depression Classification Metrics With Varied Feature Selection Methods')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balanced classification accuracy sees little change from the best performing feature-selection model to the original model. The overall classification accuracy is significantly higher with the original model than with the feature selection model, but these values would likely converge as the number of features included in the analysis increases. \n",
    "The original model performs better on classifying the male participants, with a 9% difference in the balanced classificaiton accuracy. However, the feature-selection model performs better on classifying female participants, with a 10% increase in the balanced classification accuracy. Furthermore, the Equality of Opportunity score increases 20% with the feature-selection model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (1c) Gender Feature Selection\n",
    "TODO Fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in\n",
    "\n",
    "# line graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (1d) Mitigating Bias Via Removing Gender-Dependent Features\n",
    "TODO Fill in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in\n",
    "\n",
    "# bar graph comparing to part a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (1e) Mitigating Bias Via Other Approaches\n",
    "\n",
    "#### Mitigating Bias via reweighting\n",
    "Reweight the samples between female and male speakers by computing the proportion of two classes, and also by applying 'balanced' class weight for random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample based reweight\n",
    "\n",
    "samples_per_gender = train_df['Gender'].value_counts()\n",
    "total_samples = len(train_df)\n",
    "inverse_representation = total_samples / samples_per_gender\n",
    "print(samples_per_gender)\n",
    "\n",
    "train_df_weights = train_df.copy()\n",
    "train_df_weights['weights'] = train_df['Gender'].map(inverse_representation)\n",
    "reweights = train_df_weights['weights']"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_sbr = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "model_sbr.fit(X_train, y_train_depression, sample_weight=reweights)\n",
    "\n",
    "y_pred_sbr = model_sbr.predict(X_test)\n",
    "all_sbr, male_sbr, female_sbr, eo_sbr = analyze_results(y_pred_sbr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Mitigating bias via autoencoder\n",
    "Implement autoencoder by training a multi-task neural network. Through the training, compute the loss for gender and depression, and try to decrease the loss of depression prediction while increasing the loss of gender prediction, which will make sure the output features will represent depression well but gender badly. To achieve that, use the negative value of binary cross entropy of gender prediction. After training, take the output as the transformed features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# auto-encoder \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "input_layer = Input(shape=(88,))\n",
    "encoded = Dense(44, activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "decoded = Dense(88, activation='sigmoid', name='decoded')(encoded)\n",
    "\n",
    "gender_output = Dense(1, activation='sigmoid', name='gender')(encoded)\n",
    "depression_output = Dense(1, name='depression')(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=[decoded, gender_output, depression_output])\n",
    "\n",
    "def negative_binary_crossentropy(y_true, y_pred):\n",
    "    return -tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "losses = {\n",
    "    'decoded': mean_squared_error,\n",
    "    'gender': negative_binary_crossentropy,\n",
    "    'depression': mean_squared_error,\n",
    "}\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=losses)\n",
    "\n",
    "autoencoder.fit(X_train, [X_train, y_train_gender, y_train_depression], epochs=10, batch_size=32)\n",
    "X_train_encoded = autoencoder.predict(X_train)[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_aer = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "model_aer.fit(X_train_encoded, y_train_depression)\n",
    "\n",
    "y_pred_aer = model_aer.predict(X_test)\n",
    "all_aer, male_aer, female_aer, eo_aer = analyze_results(y_pred_aer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vals = {\n",
    "    'all features': [all_metrics_depression['accuracy'], all_metrics_depression['balanced_accuracy'], \n",
    "                     male_metrics['accuracy'], male_metrics['balanced_accuracy'], \n",
    "                     female_metrics['accuracy'], female_metrics['balanced_accuracy'], .6],\n",
    "    'SBR features': [all_sbr['accuracy'], all_sbr['balanced_accuracy'], \n",
    "                     male_sbr['accuracy'], male_sbr['balanced_accuracy'], \n",
    "                     female_sbr['accuracy'], female_sbr['balanced_accuracy'], eo_sbr],\n",
    "    'AER features': [all_aer['accuracy'], all_aer['balanced_accuracy'], \n",
    "                 male_aer['accuracy'], male_aer['balanced_accuracy'], \n",
    "                 female_aer['accuracy'], female_aer['balanced_accuracy'], eo_aer]\n",
    "}\n",
    "bar_graph(vals, measures, 'Depression Classification Metrics With Debiasing')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "According to the graph, mitigating bias will reduce the accuracy of the model. This might be because the model is trained on imbalanced value, so it can get high accuracy by simply predicting the class which has more samples. Balanced accuracy would be increased through this transformation. The accuracy on male class, which has more samples, will decrease as well. It makes sense because it takes a huge percentage of the overall accuracy. Moreover, the performance of using autoencoder will mostly depend on how well the autoencoder is trained, as in this task, where the autoencoder struggles with reducing losses stably, the final performance changes a lot.  \n",
    "One interesting thing to notice is that after autoencoder, the EO could reach to 1, which indicates it a better choice of mitigating bias."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
